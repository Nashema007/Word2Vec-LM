{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nashema007/Word2Vec-LM/blob/main/NLP_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoX5ZsAieHP7",
        "outputId": "5b7305cc-8760-4253-9b91-c7708cf01ee1"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import nltk\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import sklearn\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "import re\n",
        "import time\n",
        "import math\n",
        "from collections import Counter\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "nltk.download('punkt')\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwQ8b0IG2ypB"
      },
      "source": [
        "# represents the number of context words to use after the center word\n",
        "CONTEXT_SIZE = 3\n",
        "# embedding dim can be 50, 100, 300\n",
        "EMBEDDING_DIM = 300"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1EaNA2pCeWV"
      },
      "source": [
        "def read_file(file):  \n",
        "  path = os.sys.path[1]\n",
        "  txt = open((path + file))\n",
        "  txtRead = txt.read()\n",
        "  return txtRead\n",
        "test_data_file = read_file('/nchlt_text.nr.test')\n",
        "train_data_file = read_file('/nchlt_text.nr.train')\n",
        "valid_data_file = read_file('/nchlt_text.nr.valid')\n",
        "test_data = nltk.sent_tokenize(test_data_file)\n",
        "train_data = nltk.sent_tokenize(train_data_file)\n",
        "valid_data = nltk.word_tokenize(valid_data_file)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLrVql_ceGAQ"
      },
      "source": [
        "sentences = []\n",
        "for value in train_data:\n",
        "   sentences.append(nltk.word_tokenize(re.sub(pattern =r'[\\!\"#$%&\\*+,./:;<=>?@^_`()|~=]', repl='', string = value)))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ii9eufFW1r6"
      },
      "source": [
        "all_sentences = []\n",
        "for i in sentences:\n",
        "  all_sentences+=i"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLsXXxQqQZ5N"
      },
      "source": [
        "all_sentences_dict = dict(Counter(all_sentences))\n",
        "all_sentences_dict['<unk>'] = len(all_sentences_dict) - len({x:y for x,y in all_sentences_dict.items() if y > 1})\n",
        "all_sentences = [i for i in all_sentences_dict.keys()]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HMNxcQDcocj"
      },
      "source": [
        "vocabulary = list(set(all_sentences_dict.keys())) \n",
        "vocabulary_size = len(vocabulary)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDTgY6Y72l7R"
      },
      "source": [
        "ngrams = []\n",
        "for i in range(len(all_sentences) - CONTEXT_SIZE):\n",
        "    tup = [all_sentences[j] for j in np.arange(i + 1 , i + CONTEXT_SIZE + 1) ]\n",
        "    ngrams.append((all_sentences[i],tup))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhd-xBH3i8RD"
      },
      "source": [
        "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
        "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WfbkiqA4UKQ"
      },
      "source": [
        "class SkipgramModeler(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "        super(SkipgramModeler, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
        "        self.linear2 = nn.Linear(128, context_size * vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embeds = self.embeddings(inputs).view((1, -1))  # -1 implies size inferred for that index from the size of the data\n",
        "        m = nn.Dropout(p=0.8)\n",
        "        embeds = m(embeds) # apply drop out on the first layer with .8% prob\n",
        "        out1 = F.relu(self.linear1(embeds)) # output of first layer \n",
        "        n = nn.Dropout(p=0.2) # apply drop out for second layer with .2% prob\n",
        "        out1 = n(out1)\n",
        "        out2 = self.linear2(out1)           # output of second layer\n",
        "        log_probs = F.log_softmax(out2, dim=1).view(CONTEXT_SIZE,-1)\n",
        "        return log_probs\n",
        "\n",
        "    def predict(self,input):\n",
        "        context_idxs = torch.tensor([word2idx[input]], dtype=torch.long)\n",
        "        res = self.forward(context_idxs)\n",
        "        res_arg = torch.argmax(res)\n",
        "        res_val, res_ind = res.sort(descending=True)\n",
        "        indices = [res_ind[i][0] for i in np.arange(0,CONTEXT_SIZE)]\n",
        "        for arg in indices:\n",
        "          print( [ (key, val) for key,val in word2idx.items() if val == arg ])\n",
        "    \n",
        "    def divide_chunks(self,ngrams, chunk_size):\n",
        "      # looping till length l\n",
        "      for i in range(0, len(ngrams), chunk_size): \n",
        "          yield ngrams[i:i + chunk_size]\n",
        "\n",
        "    def write_embedding_to_file(self,filename):\n",
        "        for i in self.embeddings.parameters():\n",
        "            weights = i.data.numpy()\n",
        "        np.save(filename,weights)\n",
        "    def create_log(self, text):\n",
        "      outF = open(\"model.log\", \"a\")\n",
        "      outF.write(text)\n",
        "      outF.write(\"\\n\")\n",
        "      outF.close()\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUjl-LIyBcVe"
      },
      "source": [
        "model = SkipgramModeler(vocabulary_size, EMBEDDING_DIM, CONTEXT_SIZE)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkGv-tN--O3S"
      },
      "source": [
        "def calculateLoss(ngramType,ngrams, model, lr, num_epochs, batch_size, adam=False):\n",
        "  valid_losses = []\n",
        "  valid_perpelxity = []\n",
        "  test_loss = []\n",
        "  test_perp = []\n",
        "  loss_function = nn.NLLLoss()\n",
        "  if adam:\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=True)\n",
        "  else:\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "  data = list(model.divide_chunks(ngrams, batch_size))\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "  b = 1\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    start1 = time.time()\n",
        "    total_loss = 0\n",
        "    for single_batch in data:\n",
        "      start = time.time()\n",
        "      for context, target in single_batch:\n",
        "        context_idxs = torch.tensor([word2idx[context]], dtype=torch.long)\n",
        "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
        "        # into integer indices and wrap them in tensors)\n",
        "        context_idxs = torch.tensor([word2idx[context]], dtype=torch.long)\n",
        "\n",
        "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
        "        # new instance, you need to zero out the gradients from the old\n",
        "        # instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 3. Run the forward pass, getting log probabilities over next\n",
        "        # words\n",
        "        log_probs = model.forward(context_idxs)\n",
        "\n",
        "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
        "        # word wrapped in a tensor)\n",
        "        target_list = torch.tensor([word2idx[w] for w in target], dtype=torch.long)\n",
        "\n",
        "      \n",
        "        loss = loss_function(log_probs, target_list)\n",
        "\n",
        "        # Step 5. Do the backward pass and update the gradient\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
        "        total_loss += loss.item()\n",
        "        perplexity  = torch.exp(loss)\n",
        "        valid_perpelxity.append(perplexity)\n",
        "      end = time.time()\n",
        "      scheduler.step()\n",
        "      print(ngramType+' loss')\n",
        "      textLog = f'| epoch {epoch+1} | {b}/{len(data)} batches | lr {lr} | s/batch {end-start} | loss {round(loss.item(), 3)} | ppl {perplexity}'\n",
        "      model.create_log(textLog)\n",
        "      print(textLog)\n",
        "      b += 1\n",
        "      # print(total_loss)\n",
        "    b = 1\n",
        "    end1 = time.time()\n",
        "    valid_losses.append(total_loss)\n",
        "    overall_losses = sum(valid_losses)/batch_size\n",
        "    overall_perpelxity = sum(valid_perpelxity)/batch_size\n",
        "    test_loss.append(overall_losses)\n",
        "    test_perp.append(overall_perpelxity)\n",
        "    validLog = f'| end of epoch {epoch+1} | time: {end1-start1}s | valid loss {overall_losses} | valid ppl {overall_perpelxity}'\n",
        "    print(validLog)\n",
        "    model.create_log(validLog)\n",
        "  t_p = sum(test_perp)/num_epochs\n",
        "  t_l = sum(test_loss)/num_epochs\n",
        "  testLog = f'| End of training | test loss {t_l} | test ppl {t_p}'\n",
        "  print(testLog)\n",
        "  model.create_log(testLog)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj-_Rle9eOKK",
        "outputId": "b4f1b2b4-10a5-4555-c598-e6ec29d5c97c"
      },
      "source": [
        "ngramType=''\n",
        "if CONTEXT_SIZE == 1:\n",
        "  ngramType='Unigram'\n",
        "elif CONTEXT_SIZE == 2:\n",
        "  ngramType='Bigram'\n",
        "elif CONTEXT_SIZE == 3:\n",
        "  ngramType='Trigram'\n",
        "elif CONTEXT_SIZE == 4:\n",
        "  ngramType='Four-gram'\n",
        "elif CONTEXT_SIZE == 5:\n",
        "  ngramType='Five-gram'\n",
        "calculateLoss(ngramType=ngramType, ngrams=ngrams[:5000], model=model, lr=0.05, num_epochs=3, batch_size=200)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trigram loss\n",
            "| epoch 1 | 1/25 batches | lr 0.05 | s/batch 59.549243688583374 | loss 13.861 | ppl 1046724.6875\n",
            "Trigram loss\n",
            "| epoch 1 | 2/25 batches | lr 0.05 | s/batch 58.795618772506714 | loss 14.109 | ppl 1341431.875\n",
            "Trigram loss\n",
            "| epoch 1 | 3/25 batches | lr 0.05 | s/batch 58.77241539955139 | loss 13.378 | ppl 645394.375\n",
            "Trigram loss\n",
            "| epoch 1 | 4/25 batches | lr 0.05 | s/batch 59.05006718635559 | loss 12.757 | ppl 346932.1875\n",
            "Trigram loss\n",
            "| epoch 1 | 5/25 batches | lr 0.05 | s/batch 58.85957479476929 | loss 12.373 | ppl 236398.1875\n",
            "Trigram loss\n",
            "| epoch 1 | 6/25 batches | lr 0.05 | s/batch 58.44507384300232 | loss 12.783 | ppl 356155.28125\n",
            "Trigram loss\n",
            "| epoch 1 | 7/25 batches | lr 0.05 | s/batch 57.99445652961731 | loss 12.731 | ppl 338024.0\n",
            "Trigram loss\n",
            "| epoch 1 | 8/25 batches | lr 0.05 | s/batch 58.14544987678528 | loss 13.181 | ppl 530072.9375\n",
            "Trigram loss\n",
            "| epoch 1 | 9/25 batches | lr 0.05 | s/batch 59.191744804382324 | loss 13.877 | ppl 1063741.625\n",
            "Trigram loss\n",
            "| epoch 1 | 10/25 batches | lr 0.05 | s/batch 58.92616128921509 | loss 13.385 | ppl 650251.0\n",
            "Trigram loss\n",
            "| epoch 1 | 11/25 batches | lr 0.05 | s/batch 59.13473033905029 | loss 12.682 | ppl 321873.9375\n",
            "Trigram loss\n",
            "| epoch 1 | 12/25 batches | lr 0.05 | s/batch 58.62439751625061 | loss 13.544 | ppl 762274.9375\n",
            "Trigram loss\n",
            "| epoch 1 | 13/25 batches | lr 0.05 | s/batch 58.17774677276611 | loss 12.479 | ppl 262695.09375\n",
            "Trigram loss\n",
            "| epoch 1 | 14/25 batches | lr 0.05 | s/batch 58.89263653755188 | loss 12.584 | ppl 291806.75\n",
            "Trigram loss\n",
            "| epoch 1 | 15/25 batches | lr 0.05 | s/batch 61.36102604866028 | loss 13.229 | ppl 556405.0625\n",
            "Trigram loss\n",
            "| epoch 1 | 16/25 batches | lr 0.05 | s/batch 60.95168852806091 | loss 13.884 | ppl 1070998.375\n",
            "Trigram loss\n",
            "| epoch 1 | 17/25 batches | lr 0.05 | s/batch 59.95635175704956 | loss 12.796 | ppl 360649.1875\n",
            "Trigram loss\n",
            "| epoch 1 | 18/25 batches | lr 0.05 | s/batch 60.11244034767151 | loss 14.15 | ppl 1396604.125\n",
            "Trigram loss\n",
            "| epoch 1 | 19/25 batches | lr 0.05 | s/batch 60.589165687561035 | loss 13.183 | ppl 531340.8125\n",
            "Trigram loss\n",
            "| epoch 1 | 20/25 batches | lr 0.05 | s/batch 60.03570485115051 | loss 14.469 | ppl 1922274.875\n",
            "Trigram loss\n",
            "| epoch 1 | 21/25 batches | lr 0.05 | s/batch 60.0700261592865 | loss 13.601 | ppl 807032.375\n",
            "Trigram loss\n",
            "| epoch 1 | 22/25 batches | lr 0.05 | s/batch 59.93475341796875 | loss 13.006 | ppl 444933.0625\n",
            "Trigram loss\n",
            "| epoch 1 | 23/25 batches | lr 0.05 | s/batch 60.597824811935425 | loss 14.046 | ppl 1258969.375\n",
            "Trigram loss\n",
            "| epoch 1 | 24/25 batches | lr 0.05 | s/batch 60.44134473800659 | loss 12.744 | ppl 342486.6875\n",
            "Trigram loss\n",
            "| epoch 1 | 25/25 batches | lr 0.05 | s/batch 61.55843257904053 | loss 13.177 | ppl 527997.3125\n",
            "| end of epoch 1 | time: 1488.213608264923s | valid loss 330.1945296669006 | valid ppl 17250514.0\n",
            "Trigram loss\n",
            "| epoch 2 | 1/25 batches | lr 0.05 | s/batch 60.72243309020996 | loss 12.126 | ppl 184541.640625\n",
            "Trigram loss\n",
            "| epoch 2 | 2/25 batches | lr 0.05 | s/batch 60.58663725852966 | loss 10.503 | ppl 36421.3203125\n",
            "Trigram loss\n",
            "| epoch 2 | 3/25 batches | lr 0.05 | s/batch 60.50053429603577 | loss 8.785 | ppl 6535.80859375\n",
            "Trigram loss\n",
            "| epoch 2 | 4/25 batches | lr 0.05 | s/batch 60.081045389175415 | loss 12.707 | ppl 330150.84375\n",
            "Trigram loss\n",
            "| epoch 2 | 5/25 batches | lr 0.05 | s/batch 59.840012311935425 | loss 12.001 | ppl 162875.4375\n",
            "Trigram loss\n",
            "| epoch 2 | 6/25 batches | lr 0.05 | s/batch 59.80173063278198 | loss 12.668 | ppl 317401.5625\n",
            "Trigram loss\n",
            "| epoch 2 | 7/25 batches | lr 0.05 | s/batch 59.41265606880188 | loss 12.961 | ppl 425535.0\n",
            "Trigram loss\n",
            "| epoch 2 | 8/25 batches | lr 0.05 | s/batch 59.71830105781555 | loss 13.436 | ppl 683974.3125\n",
            "Trigram loss\n",
            "| epoch 2 | 9/25 batches | lr 0.05 | s/batch 61.01824760437012 | loss 12.775 | ppl 353150.84375\n",
            "Trigram loss\n",
            "| epoch 2 | 10/25 batches | lr 0.05 | s/batch 61.141359090805054 | loss 13.599 | ppl 805196.5625\n",
            "Trigram loss\n",
            "| epoch 2 | 11/25 batches | lr 0.05 | s/batch 60.81354212760925 | loss 13.23 | ppl 556633.8125\n",
            "Trigram loss\n",
            "| epoch 2 | 12/25 batches | lr 0.05 | s/batch 62.578938007354736 | loss 12.621 | ppl 302827.6875\n",
            "Trigram loss\n",
            "| epoch 2 | 13/25 batches | lr 0.05 | s/batch 62.12264370918274 | loss 13.902 | ppl 1090348.625\n",
            "Trigram loss\n",
            "| epoch 2 | 14/25 batches | lr 0.05 | s/batch 61.84184408187866 | loss 13.996 | ppl 1198226.5\n",
            "Trigram loss\n",
            "| epoch 2 | 15/25 batches | lr 0.05 | s/batch 62.29219365119934 | loss 13.907 | ppl 1095631.25\n",
            "Trigram loss\n",
            "| epoch 2 | 16/25 batches | lr 0.05 | s/batch 61.60965609550476 | loss 13.602 | ppl 807553.5625\n",
            "Trigram loss\n",
            "| epoch 2 | 17/25 batches | lr 0.05 | s/batch 61.930657386779785 | loss 12.9 | ppl 400438.03125\n",
            "Trigram loss\n",
            "| epoch 2 | 18/25 batches | lr 0.05 | s/batch 61.37513589859009 | loss 14.103 | ppl 1332872.75\n",
            "Trigram loss\n",
            "| epoch 2 | 19/25 batches | lr 0.05 | s/batch 61.577542543411255 | loss 14.585 | ppl 2159065.5\n",
            "Trigram loss\n",
            "| epoch 2 | 20/25 batches | lr 0.05 | s/batch 62.043031215667725 | loss 13.876 | ppl 1062454.125\n",
            "Trigram loss\n",
            "| epoch 2 | 21/25 batches | lr 0.05 | s/batch 61.17856478691101 | loss 13.052 | ppl 465812.9375\n",
            "Trigram loss\n",
            "| epoch 2 | 22/25 batches | lr 0.05 | s/batch 60.06446051597595 | loss 11.976 | ppl 158962.296875\n",
            "Trigram loss\n",
            "| epoch 2 | 23/25 batches | lr 0.05 | s/batch 59.92639946937561 | loss 12.544 | ppl 280444.5625\n",
            "Trigram loss\n",
            "| epoch 2 | 24/25 batches | lr 0.05 | s/batch 60.100764989852905 | loss 13.345 | ppl 624441.5625\n",
            "Trigram loss\n",
            "| epoch 2 | 25/25 batches | lr 0.05 | s/batch 60.22584795951843 | loss 12.483 | ppl 263807.78125\n",
            "| end of epoch 2 | time: 1522.54820895195s | valid loss 653.4800598478317 | valid ppl 32663596.0\n",
            "Trigram loss\n",
            "| epoch 3 | 1/25 batches | lr 0.05 | s/batch 60.2403244972229 | loss 10.419 | ppl 33480.2734375\n",
            "Trigram loss\n",
            "| epoch 3 | 2/25 batches | lr 0.05 | s/batch 60.43157887458801 | loss 9.723 | ppl 16702.302734375\n",
            "Trigram loss\n",
            "| epoch 3 | 3/25 batches | lr 0.05 | s/batch 60.235575675964355 | loss 11.391 | ppl 88510.78125\n",
            "Trigram loss\n",
            "| epoch 3 | 4/25 batches | lr 0.05 | s/batch 59.45512819290161 | loss 13.455 | ppl 697641.4375\n",
            "Trigram loss\n",
            "| epoch 3 | 5/25 batches | lr 0.05 | s/batch 59.44370198249817 | loss 13.617 | ppl 819586.5\n",
            "Trigram loss\n",
            "| epoch 3 | 6/25 batches | lr 0.05 | s/batch 59.713308334350586 | loss 13.008 | ppl 445747.21875\n",
            "Trigram loss\n",
            "| epoch 3 | 7/25 batches | lr 0.05 | s/batch 59.307737588882446 | loss 13.104 | ppl 491145.1875\n",
            "Trigram loss\n",
            "| epoch 3 | 8/25 batches | lr 0.05 | s/batch 59.38407301902771 | loss 12.763 | ppl 349005.875\n",
            "Trigram loss\n",
            "| epoch 3 | 9/25 batches | lr 0.05 | s/batch 59.33648204803467 | loss 13.778 | ppl 963569.0625\n",
            "Trigram loss\n",
            "| epoch 3 | 10/25 batches | lr 0.05 | s/batch 59.8623411655426 | loss 13.465 | ppl 703997.4375\n",
            "Trigram loss\n",
            "| epoch 3 | 11/25 batches | lr 0.05 | s/batch 59.67195177078247 | loss 12.766 | ppl 350248.59375\n",
            "Trigram loss\n",
            "| epoch 3 | 12/25 batches | lr 0.05 | s/batch 59.03429651260376 | loss 13.763 | ppl 948804.9375\n",
            "Trigram loss\n",
            "| epoch 3 | 13/25 batches | lr 0.05 | s/batch 58.88285565376282 | loss 14.091 | ppl 1317573.0\n",
            "Trigram loss\n",
            "| epoch 3 | 14/25 batches | lr 0.05 | s/batch 59.09814214706421 | loss 13.068 | ppl 473451.75\n",
            "Trigram loss\n",
            "| epoch 3 | 15/25 batches | lr 0.05 | s/batch 59.51287603378296 | loss 13.016 | ppl 449374.8125\n",
            "Trigram loss\n",
            "| epoch 3 | 16/25 batches | lr 0.05 | s/batch 59.22291398048401 | loss 13.753 | ppl 938950.1875\n",
            "Trigram loss\n",
            "| epoch 3 | 17/25 batches | lr 0.05 | s/batch 59.320473432540894 | loss 12.616 | ppl 301409.84375\n",
            "Trigram loss\n",
            "| epoch 3 | 18/25 batches | lr 0.05 | s/batch 59.629591941833496 | loss 12.862 | ppl 385358.71875\n",
            "Trigram loss\n",
            "| epoch 3 | 19/25 batches | lr 0.05 | s/batch 58.9883759021759 | loss 13.964 | ppl 1160337.0\n",
            "Trigram loss\n",
            "| epoch 3 | 20/25 batches | lr 0.05 | s/batch 59.26222586631775 | loss 13.726 | ppl 914827.6875\n",
            "Trigram loss\n",
            "| epoch 3 | 21/25 batches | lr 0.05 | s/batch 60.41328549385071 | loss 14.152 | ppl 1399976.625\n",
            "Trigram loss\n",
            "| epoch 3 | 22/25 batches | lr 0.05 | s/batch 64.09343957901001 | loss 13.18 | ppl 529849.5625\n",
            "Trigram loss\n",
            "| epoch 3 | 23/25 batches | lr 0.05 | s/batch 60.395939111709595 | loss 13.365 | ppl 637539.625\n",
            "Trigram loss\n",
            "| epoch 3 | 24/25 batches | lr 0.05 | s/batch 59.78095602989197 | loss 13.004 | ppl 444287.71875\n",
            "Trigram loss\n",
            "| epoch 3 | 25/25 batches | lr 0.05 | s/batch 61.002299308776855 | loss 14.188 | ppl 1451304.0\n",
            "| end of epoch 3 | time: 1495.7639932632446s | valid loss 976.6523861670494 | valid ppl 48100336.0\n",
            "| End of training | test loss 653.4423252272605 | test ppl 32671482.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw2jAUVWVBTH"
      },
      "source": [
        "def cluster_embeddings(filename,nclusters):\n",
        "    X = np.load(filename)\n",
        "    kmeans = KMeans(n_clusters=nclusters, random_state=0).fit(X)\n",
        "    center = kmeans.cluster_centers_\n",
        "    distances = euclidean_distances(X,center)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_fGTX_xkqnj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a187b2bd-b059-48b1-9742-f3c6e42aca8e"
      },
      "source": [
        "#Predict the next word given n context words\n",
        "model.predict('kobana')\n",
        "model.write_embedding_to_file('embeddings_skipgrams.npy')\n",
        "cluster_embeddings('embeddings_skipgrams.npy',5)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('amalungelo', 28765)]\n",
            "[('Ngiyamukela', 12416)]\n",
            "[('ngesikhathi', 53837)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}